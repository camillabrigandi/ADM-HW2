{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efd9fef8",
   "metadata": {},
   "source": [
    "# ADM Homework 2 - Research Questions (RQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8f0acd",
   "metadata": {},
   "source": [
    "## Preliminary actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section of the notebook contains all the cleaning operations on the datasets. Originally, they have been implemented while solving the research questions. Every question made us work on different attributes of the datasets and allowed us to understand in detail all the characteristics of the attributes, such as anomalies and empty values. For clarity issues, we decided to perform most of these operations at the beginning of the notebook under the \"Preliminary actions\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f10e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import math \n",
    "import calendar\n",
    "import re\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7cf5911",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_json('lighter_authors.json', lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f737dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = pd.read_json(\"lighter_books.json\", lines = True, chunksize = 100000)\n",
    "df_2 = []\n",
    "for i in chunk:\n",
    "    i = i.drop([\"isbn\"], axis = 1)\n",
    "    i = i.drop([\"authors\"], axis = 1)\n",
    "    i = i.drop([\"isbn13\"], axis = 1)\n",
    "    i = i.drop([\"image_url\"], axis = 1)\n",
    "    i = i.drop([\"description\"], axis = 1)\n",
    "    i = i.drop([\"shelves\"], axis = 1)\n",
    "    i = i.drop([\"asin\"], axis = 1)\n",
    "    i = i.drop([\"series_position\"], axis = 1)\n",
    "    df_2.append(i)\n",
    "df_2 = pd.concat(df_2, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1651796f",
   "metadata": {},
   "source": [
    "This code has been implemented in order to manage the original 'ligher_books' file, that was impossible to open in its original version, because of memory issues. It reads the file in chunks of 100.000 rows at a time while we remove 8 columns to make the whole dataset lighter. The average time to open it with this method is around 10 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**df_1 (lighter_authors) - preliminary cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING 1: dropping not-needed columns\n",
    "df_1.drop(['image_url','about'], axis = 1, inplace=True)\n",
    "\n",
    "#CLEANING 2: operations on \"name\" column\n",
    "df_1['name'] = df_1['name'].str.strip().replace(['Source Wikipedia', 'Source Wikia', 'Fuente Wikipedia'], 'Wikipedia')\n",
    "df_1 = df_1[df_1.name != 'NOT A BOOK']\n",
    "df_1 = df_1[df_1.name != 'Unknown']\n",
    "df_1 = df_1[df_1.name != 'Various'] \n",
    "df_1 = df_1[df_1.name != 'Anonymous']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**df_2 (lighter_books) - preliminary cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING 1: setting appropriate avararge rating interval\n",
    "df_2 = df_2[ (df_2.average_rating >= 1) & (df_2.average_rating <= 5)]\n",
    "\n",
    "#CLEANING 2: operations on \"name\" column\n",
    "df_2 = df_2[df_2.author_name != 'NOT A BOOK']\n",
    "df_2 = df_2[df_2.author_name != 'Unknown']\n",
    "df_2 = df_2[df_2.author_name != 'Various']\n",
    "df_2 = df_2[df_2.author_name != 'Anonymous']\n",
    "df_2['author_name'] = df_2['author_name'].str.strip().replace(['Source Wikipedia', 'Source Wikia', 'Fuente Wikipedia'], 'Wikipedia')\n",
    "\n",
    "#CLEANING 3: converting \"num_pages\" values into numbers\n",
    "df_2['num_pages'] = pd.to_numeric(df_2['num_pages'], errors='coerce').fillna(0).astype(np.int64)\n",
    "df_2 = df_2[df_2['num_pages'] > 0]\n",
    "\n",
    "#CLEANING 4: operations on \"rating_dist\" column\n",
    "df_2['rating_dist'] = df_2['rating_dist'].apply(lambda row: re.split(r':|\\|', row))\n",
    "df_2['rating_dist'] = df_2['rating_dist'].apply(lambda row: row[1::2])\n",
    "df_2['rating_dist'] = df_2['rating_dist'].apply(lambda row: [int(e) for e in row])\n",
    "keys=[5, 4, 3, 2, 1, 'total']\n",
    "df_2['rating_dist'] = df_2['rating_dist'].apply(lambda row: dict(zip(keys, row)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLEANING OPERATIONS - DF 1 (lighter_authors):**\n",
    "1) We decided to drop the attributes \"image_url\" and \"about\" for memory issues and because we didn't need them;\n",
    "2) The logic behind our choice on this part of the code is to consider \"Wikipedia\" as an author, while dropping \"Unknown\", \"Various\", \"Anonymous\" and \"NOT A BOOK\", not considering them as authors. \n",
    "\n",
    "**CLEANING OPERATIONS - DF 2 (lighter_books):**\n",
    "1) The logic is the same for the point \"1.\" of the previous cleaning operation;\n",
    "2) The logic is the same for the point \"3.\" of the previous cleaning operation;\n",
    "3) We converted the values in \"num_pages\" into integers to be able to manipulate them;\n",
    "4) We noticed that the \"rating_dist\" column was stored as a string, and we converted it into a dictionary to be able to access the informaion inside it in a convenient way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c8357e",
   "metadata": {},
   "source": [
    "# RQ 1 - *Exploratory Data Analysis (EDA)* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lighter_authors EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e470b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#here we print all the attributes and the statistics of the dataset \n",
    "df_1.info()\n",
    "summary_statistics_df_1 = df_1.describe()\n",
    "print(summary_statistics_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ba8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we print how a row looks like \n",
    "df_1.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we search the max and min average ratings\n",
    "max_avg_rating = df_1['average_rating'].max()\n",
    "min_avg_rating = df_1['average_rating'].min()\n",
    "print(max_avg_rating)\n",
    "print(min_avg_rating)\n",
    "\n",
    "#we find out that there is a rating that is -31 and this can't be possible because the ratings go from 1 to 5, so now we delete every rating that is <1\n",
    "\n",
    "# setting appropriate avararge rating interval\n",
    "df_1 = df_1[ (df_1.average_rating >= 1) & (df_1.average_rating <= 5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed that there were values under the \"average_rating\" column outside the interval $[1, 5]$, which is the correct interval of accepted rating values, so we dropped everything outside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_1['average_rating'], bins=20, color='blue', edgecolor='black')\n",
    "plt.xlabel('Average Ratings')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Average Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_1['text_reviews_count'], bins=20, color='blue', edgecolor='black')\n",
    "plt.xlabel('Text reviews')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of text reviews')\n",
    "plt.yscale(value='log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4757fca5",
   "metadata": {},
   "source": [
    "## lighter_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here we print all the attributes and the statistics of the dataset \n",
    "df_2.info()\n",
    "summary_statistics_df_2 = df_2.describe()\n",
    "print(summary_statistics_df_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to find out if there are duplicate rows\n",
    "columns_to_check = ['title']\n",
    "duplicate_rows = df_2[df_2.duplicated(subset=columns_to_check, keep=False)]\n",
    "num_duplicate_rows = duplicate_rows.shape[0]\n",
    "print(f\"Number of duplicate rows: {num_duplicate_rows}\")\n",
    "print(duplicate_rows.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3025fe5d",
   "metadata": {},
   "source": [
    "# RQ2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324d754",
   "metadata": {},
   "source": [
    "## RQ2.1 Plot the number of books for each author in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048018ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rq21 = df_2[['title', 'author_id', 'work_id','author_name']]\n",
    "df_rq21 = df_rq21.drop_duplicates()\n",
    "author_books_count = df_rq21.groupby('author_id').agg({'work_id':'count', 'author_name':'first'}).reset_index()\n",
    "author_books_count.rename(columns={'work_id': 'num_books', 'author_name':'author_name'}, inplace=True)\n",
    "author_books_count = author_books_count.sort_values(by='num_books', ascending=False)\n",
    "print(\"The number of books for each author in descending order is\", author_books_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173401cd",
   "metadata": {},
   "source": [
    "## RQ2.2 Which book has the highest number of reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85770b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_reviews_count = df_2['text_reviews_count'].max()\n",
    "max_row = df_2[df_2['text_reviews_count'] == max_text_reviews_count]\n",
    "max_title = max_row['title'].values[0]\n",
    "print(\"The book with the highest number of reviews is:\", max_title, \"with \", max_text_reviews_count, \"reviews.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c6f17b",
   "metadata": {},
   "source": [
    "## RQ2.3 Which are the top ten and ten worst books concerning the average score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0482c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOP 10\n",
    "\n",
    "#all_5_ratings_books = len(df_2[df_2['average_rating']==5.0])\n",
    "#we have seen that the books with an average rating of 5 are 231.276\n",
    "\n",
    "df_2_ratings_5 = df_2[df_2['average_rating']==5.0]\n",
    "df_highest_avg_score = df_2_ratings_5.nlargest(10, 'ratings_count')\n",
    "\n",
    "df_highest_avg_score[['title', 'ratings_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5853f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORST 10\n",
    "\n",
    "df_d = df_2[(df_2['average_rating'] <= 1) & (df_2['ratings_count'] > 0)]\n",
    "df_lowest_avg_score = df_d.nlargest(10, 'ratings_count')\n",
    "\n",
    "df_lowest_avg_score[['title', 'ratings_count', 'average_rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681585d4",
   "metadata": {},
   "source": [
    "## RQ2.4 Explore the different languages in the book’s dataset, providing a proper chart summarizing how these languages are distributed throughout our virtual library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc6e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2_languages = df_2[df_2['language']!='']\n",
    "df_2_languages.loc[:,'language'] = df_2_languages['language'].replace(['en-US', 'en-GB'], 'eng')\n",
    "language_counts = df_2_languages['language'].value_counts()\n",
    "min_percentage = 1\n",
    "languages = language_counts[language_counts / language_counts.sum() * 100 >= min_percentage]\n",
    "table = pd.DataFrame({'Language': languages.index, 'Percentage': languages.values / language_counts.sum() * 100})\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.pie(languages, labels=languages.index, autopct='%1.1f%%')\n",
    "plt.title(\"Languages Distribution\")\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.axis('off')\n",
    "table_formatted = table.copy()\n",
    "table_formatted['Percentage'] = table_formatted['Percentage'].apply(lambda x: f'{x:.1f}%')\n",
    "table = plt.table(cellText=table_formatted.values, colLabels=table.columns, loc='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd694ca7",
   "metadata": {},
   "source": [
    "## RQ2.5 How many books have more than 250 pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed45aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pages = df_2[df_2['num_pages'] >= 250]\n",
    "print(\"The books with more than 250 pages are:\", len(df_pages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a0032",
   "metadata": {},
   "source": [
    "## RQ2.6 Plot the distribution of the fans count for the 50 most prolific authors (the ones who have written more books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e83802",
   "metadata": {},
   "outputs": [],
   "source": [
    "#barra di wikipedia è più blu delle altre\n",
    "df_prolific_authors = df_1.nlargest(50, 'works_count')\n",
    "fans_count = df_prolific_authors['fans_count']\n",
    "authors = df_prolific_authors['name']\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(authors, fans_count, color='blue', alpha=0.7)\n",
    "plt.yscale(value='log')\n",
    "plt.xlabel('Authors')\n",
    "plt.ylabel('Fans Count')\n",
    "plt.title('Fans Distribution')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, at the beginning we wanted to keep all the years of publication (starting from 0, cause there was a suggestion on slack to drop all the negative years of publication), but we noticed that for the first years there were a lot of misplaced informations (like book about people that weren't even born in that year, as one on William Shakespear \"published\" in year 0), so we decided to filter our dataset more. To do so, we decided to keep only the books published in or after 1455, which is the year of publication of Gutenberg's Bible, known to be the first ever book printed and published. For a more obvious reason, we also kept only the books published by the current year, 2023. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a lighter df to solve this question \n",
    "df_rq3 = df_2[['title','num_pages','publication_date']]\n",
    "\n",
    "#pre-processing 'publication_date' column \n",
    "pattern = r'(^[0-9]{4}-[0-9]{2}-[0-9]{1,2}$)|(^[0-9]{4}$)|(^[0-9]{4}-[0-9]{1,2}$)'\n",
    "df_rq3= df_rq3[df_rq3['publication_date'].apply(lambda row: bool(re.search(pattern, row)))]\n",
    "df_rq3['publication_date'] = df_rq3['publication_date'].apply(lambda row: re.split('-', row)) #this way we obtain year=df_rq3['original_publication_date'][0], month=df_rq3['original_publication_date'][1]\n",
    "df_rq3['publication_year'] = df_rq3['publication_date'].apply(lambda row: int(row[0]))\n",
    "\n",
    "#selecting books published in the range of years 1455-2023 as explained above\n",
    "df_rq3= df_rq3[ df_rq3['publication_year'] <= 2023]\n",
    "df_rq3= df_rq3[ df_rq3['publication_year'] >= 1455]\n",
    "\n",
    "#cleaning data\n",
    "df_rq3 = df_rq3[~df_rq3['publication_date'].isnull()]\n",
    "df_rq3 = df_rq3[~df_rq3['publication_year'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3.1: Write a function that takes as input a year and returns as output the following information:   \n",
    "- The number of books published that year.\n",
    "- The total number of pages written that year.\n",
    "- The most prolific month of that year.\n",
    "- The longest book written that year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rq3(year):\n",
    "\n",
    "    if year in list(df_rq3['publication_year']):\n",
    "        df_filtered = df_rq3[df_rq3['publication_year'] == year]\n",
    "        \n",
    "        #books published in that year\n",
    "        books_published = len(df_filtered) \n",
    "\n",
    "        #total number of pages written \n",
    "        pages_written = df_filtered['num_pages'].sum()\n",
    "\n",
    "        #most prolific mont\n",
    "        #considering the books with the months registered\n",
    "        hasmonth=[(len(date)>1) for date in df_filtered['publication_date']]\n",
    "        \n",
    "        if True in hasmonth:    \n",
    "            df_filtered_withmonth = df_filtered[hasmonth]\n",
    "            df_filtered_withmonth['month'] = df_filtered_withmonth.publication_date.apply(lambda row: int(row[1]))\n",
    "            df_filtered_withmonth = df_filtered_withmonth[df_filtered_withmonth['month'] <= 12]\n",
    "            \n",
    "            grouped = df_filtered_withmonth.groupby(by='month')\n",
    "            monthly_counts = grouped['title'].count()\n",
    "            most_prolific_month = calendar.month_name[monthly_counts.idxmax()]\n",
    "        #for the books with no month of publication registered\n",
    "        else:  \n",
    "            most_prolific_month= 'No publication month registered for this year'\n",
    "\n",
    "        #longest book written in that year\n",
    "        longest_book = df_filtered['title'][df_filtered['num_pages'].idxmax()]\n",
    "        \n",
    "        #output format:tuple\n",
    "        return books_published, pages_written, most_prolific_month, longest_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3.2: Use this function to build your data frame: the primary key will be a year, and the required information will be the attributes within the row. Finally, show the head and the tail of this new data frame considering the first ten years registered and the last ten years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_year = df_rq3.publication_year.min()\n",
    "last_year = 2023 \n",
    "\n",
    "#getting a list with all the years registered in \"publication_year\" without duplicates\n",
    "years = [ year for year in range(first_year, last_year + 1) if year in list(df_rq3['publication_year']) ] \n",
    "\n",
    "\n",
    "d = []\n",
    "for year in years:\n",
    "    attributes = rq3(year)\n",
    "    d.append(\n",
    "        {\n",
    "            'books_published': attributes[0],\n",
    "            'pages_written': attributes[1],\n",
    "            'most_prolific_month':  attributes[2],\n",
    "            'longest_book': attributes[3]\n",
    "        }\n",
    "    )\n",
    "\n",
    "df_rq3_final = pd.DataFrame(d, index = years, columns= ['books_published', 'pages_written', 'most_prolific_month', 'longest_book'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rq3_final.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rq3_final.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ3.3: Ask ChatGPT or any other LLM chatbot tool to implement this function and compare your work with the one the bot gave you as an answer. Does the chatbot implementation work? Please test it out and verify the correctness of the implementation, explaining the process you followed to prove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one below is the code provided from ChatGPT 3.5 after the following request: \n",
    "\n",
    "\"*I have a dataset created by reducing the files \"books.json\" and \"authors.json\" gotten from this website: https://www.kaggle.com/datasets/opalskies/large-books-metadata-dataset-50-mill-entries* \\\n",
    "*I have to fullfill this request: \\\n",
    "Write a function that takes as input a year and returns as output the following information:\\\n",
    "-The number of books published that year.\\\n",
    "-The total number of pages written that year.\\\n",
    "-The most prolific month of that year.\\\n",
    "-The longest book written that year.*\n",
    "\n",
    "*Use this function to build your data frame: the primary key will be a year, and the required information will be the attributes within the row. Finally, show the head and the tail of this new data frame considering the first ten years registered and the last ten years.*\n",
    "\n",
    "*How would you solve this question?*\"\n",
    "\n",
    "On a sidenote, we added the comment \"## DO NOT RUN THIS CELL ##\" at the beginning of the cell as a warning, of course it wasn't provided by ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT RUN THIS CELL ##\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "# Load the \"books.json\" and \"authors.json\" data\n",
    "books_data = pd.read_json(\"books.json\")\n",
    "authors_data = pd.read_json(\"authors.json\")\n",
    "\n",
    "# Merge the data to create a comprehensive dataset\n",
    "data = pd.merge(books_data, authors_data, on=\"book_id\")\n",
    "\n",
    "# Define a function to get the required information for a specific year\n",
    "def get_year_info(year):\n",
    "    books_published = len(data[data[\"publication_date\"].str.contains(str(year))])\n",
    "    total_pages_written = data[data[\"publication_date\"].str.contains(str(year))][\"num_pages\"].sum()\n",
    "    most_prolific_month = data[data[\"publication_date\"].str.contains(str(year))][\"publication_date\"].str.split('-').str[1].mode().iloc[0]\n",
    "    longest_book = data[data[\"publication_date\"].str.contains(str(year))][\"num_pages\"].idxmax()\n",
    "    return books_published, total_pages_written, most_prolific_month, data.loc[longest_book][\"title\"]\n",
    "\n",
    "# Create a DataFrame for the years, using the function to get the required information\n",
    "years = range(2010, 2024)  # Assuming you want data for years 2010 to 2023\n",
    "year_info = []\n",
    "\n",
    "for year in years:\n",
    "    info = get_year_info(year)\n",
    "    year_info.append({\"Year\": year, \"Books Published\": info[0], \"Total Pages Written\": info[1], \"Most Prolific Month\": info[2], \"Longest Book\": info[3]})\n",
    "\n",
    "year_df = pd.DataFrame(year_info)\n",
    "\n",
    "# Display the head and tail of the new DataFrame\n",
    "print(\"First ten years:\")\n",
    "print(year_df.head(10))\n",
    "print(\"\\nLast ten years:\")\n",
    "print(year_df.tail(10))    # Load the \"books.json\" and \"authors.json\" data\n",
    "books_data = pd.read_json(\"books.json\")\n",
    "authors_data = pd.read_json(\"authors.json\")\n",
    "\n",
    "# Merge the data to create a comprehensive dataset\n",
    "data = pd.merge(books_data, authors_data, on=\"book_id\")\n",
    "\n",
    "# Define a function to get the required information for a specific year\n",
    "def get_year_info(year):\n",
    "    books_published = len(data[data[\"publication_date\"].str.contains(str(year))])\n",
    "    total_pages_written = data[data[\"publication_date\"].str.contains(str(year))][\"num_pages\"].sum()\n",
    "    most_prolific_month = data[data[\"publication_date\"].str.contains(str(year))][\"publication_date\"].str.split('-').str[1].mode().iloc[0]\n",
    "    longest_book = data[data[\"publication_date\"].str.contains(str(year))][\"num_pages\"].idxmax()\n",
    "    return books_published, total_pages_written, most_prolific_month, data.loc[longest_book][\"title\"]\n",
    "\n",
    "# Create a DataFrame for the years, using the function to get the required information\n",
    "years = range(2010, 2024)  # Assuming you want data for years 2010 to 2023\n",
    "year_info = []\n",
    "\n",
    "for year in years:\n",
    "    info = get_year_info(year)\n",
    "    year_info.append({\"Year\": year, \"Books Published\": info[0], \"Total Pages Written\": info[1], \"Most Prolific Month\": info[2], \"Longest Book\": info[3]})\n",
    "\n",
    "year_df = pd.DataFrame(year_info)\n",
    "\n",
    "# Display the head and tail of the new DataFrame\n",
    "print(\"First ten years:\")\n",
    "print(year_df.head(10))\n",
    "print(\"\\nLast ten years:\")\n",
    "print(year_df.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let us say that the code doesn't work. There are many reasons why this appens. Here are the ones we spotted: \n",
    "- the files' names are wrong, but this error follows directly from the prompt I wrote: in that prompt I specifically mentioned the names of the files from which our dataset is obtained, but I ddidn't mention the current files' names;\n",
    "- In line *9* of the code, there's a try to merge the authors' and the books' dataframe, but the specified column name (*book_id*) doesn't exist in any of the two datasets; morover this operation isn't necessary at all for the purpose of this question;\n",
    "- In line *14*, the operation  *.sum()* won't work brecause the values of *'num_pages'* column are stored as strings, not as integers (or any numeric value), so they need to be converted to the right format before running this code;\n",
    "- In line *15* there are at least two problems: when calling *.str[1]* we could get an error for the books that have no date of publication saved, or only the year of their publication and not the full date; when calling *.mode()* we won't get any value for the mode if there is only one publication for a given month, leading to wrong clasifications of the years in wich there's only one book per month with the full registered publication date;\n",
    "- In line *27*, when creating the resulting final dataframe, there is no \"index\" parameter specified, so the resulting dataframe will not have the years as indeces as requested, but it will have the \"implicit indeces\" as the explicit ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ4.1: You should be sure there are no eponymous (different authors who have precisely the same name) in the author's dataset. Is it true?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing all non alphabetic characters from the beginnin & end of the string\n",
    "df_ep = df_1\n",
    "pattern = r'([\\W_0-9\\-]+$)|(^[\\W_0-9\\-]+)'\n",
    "df_ep['plainname'] = df_ep.name.apply(lambda row: re.sub(pattern, '', row)) \n",
    "\n",
    "#removing all digits \n",
    "pattern = r'[0-9]|\\W^ '\n",
    "df_ep['plainname'] = df_ep['plainname'].apply(lambda row: re.sub(pattern, '', row)) \n",
    "\n",
    "#dropping rows with non valid names\n",
    "df_ep = df_ep[df_ep.plainname != '']\n",
    "\n",
    "#converting the string in lower characters only & splitting the name ([name, surname] & similia)\n",
    "df_ep['plainname'] = df_ep.plainname.str.lower().str.split()\n",
    "\n",
    "#sorting the name in order to get as eponymous the names with inverted name 6 surname order\n",
    "df_ep['plainname'] = df_ep.plainname.apply(sorted)\n",
    "df_ep['plainname'] = df_ep.plainname.apply(lambda row: ' '.join(row))\n",
    "\n",
    "#Results\n",
    "print(\"There are \", df_ep['plainname'].duplicated().sum(), \"eponymous in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get a Data Frame free from eponymous, we can run the following code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a dataframe free from eponymous\n",
    "df_no_ep = df_ep.drop_duplicates(subset='plainname')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ4.2: Write a function that, given a list of author_id, outputs a dictionary where each author_id is a key, and the related value is a list with the names of all the books the author has written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FUNCTION THAT CREATES A DICTIONARY WITH AUTHORS_ID AS A KEY AND HIS/HERS BOOKS AS VALUES\n",
    "def rq4(author_id_list):\n",
    "\n",
    "    df_filt = df_2[['author_id', 'title']]\n",
    "    df_filt = df_filt[ df_filt['author_id'].isin(author_id_list) ]   #keeping only the rows with author_id in author_id_list\n",
    "    df_filt = df_filt.groupby(df_filt['author_id'], group_keys=False)['title'].agg(list)  \n",
    "    \n",
    "    d = df_filt.to_dict()\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ4.3: What is the longest book title among the books of the top 20 authors regarding their average rating? Is it the longest book title overall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LONGEST BOOK TITLE AMONG THE BOOKS OF THE TOP 20 AUTHORS REGARDING THEIR AVERAGE RATING\n",
    "\n",
    "#sorting the authors in descending order & saving only the first 20 rows of the needed data \n",
    "df_top_ratings = df_1[['id', 'average_rating']].sort_values(by= 'average_rating', ascending=False, ignore_index=True).iloc[:21]\n",
    "\n",
    "author_id_list = list(df_top_ratings['id'])        #id in df_1 (authors) correponds to author_id in df_2 (books)\n",
    "dict_authors_titles = rq4(author_id_list)\n",
    "\n",
    "#getting all the titles of books written by the 20 top authors + flattening it \n",
    "titles = list(dict_authors_titles.values())\n",
    "\n",
    "titles_flat = []\n",
    "for aut_titles in titles:\n",
    "    titles_flat += aut_titles\n",
    "\n",
    "\n",
    "#getting longest title & its length (among 20 top authors' books)\n",
    "titles_len_top20 = [len(title) for title in titles_flat]\n",
    "max_titles_len_top20 = max(titles_len_top20)\n",
    "longest_title_top20 = titles_flat[titles_len_top20.index(max_titles_len_top20)]\n",
    "\n",
    "print(\"The longest title among the books written by the 20 top authors is \" + longest_title_top20)\n",
    "\n",
    "\n",
    "#IS IT THE LONGEST TITLE AMONG ALL THE BOOKS?\n",
    "\n",
    "#getting the longest title among all the books \n",
    "df_2['title_len'] = df_2.title.apply(lambda row:len(row))\n",
    "\n",
    "longest_title_len_all = df_2.title_len.max()\n",
    "index_longest_title = df_2.title_len.idxmax()\n",
    "longest_title = df_2['title'].loc[index_longest_title]\n",
    "\n",
    "if longest_title_top20 == longest_title_len_all:\n",
    "    if longest_title == longest_title_top20: \n",
    "        print(\"The longest title among all the books' ones is the same as the longest among the books of the 20 top authors \")\n",
    "    else: \n",
    "        print(\"The  two titles selected by the algorithm are not the same, but they have the same length\")\n",
    "else: \n",
    "    print('They are not the same, the longest title among all books is \"' + longest_title +'\"')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of the question, we notice that the title is actually incomplete, and looking at the dataset, we can see that there are come other titles in the same situation.    \n",
    "We think that this is due to a limit in the numbers of characters accetable to write the tilte of the book (255 characters). To address this problem, we provide a code that filters the dataset excluding such titles and returns the longet title among the complete ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the complete longest title among all the books \n",
    "df_2['title_len'] = df_2.title.apply(lambda row:len(row))\n",
    "df_longest_title = df_2[df_2.title_len < 255]\n",
    "\n",
    "longest_title_len_all = df_longest_title.title_len.max()\n",
    "index_longest_title = df_longest_title.title_len.idxmax()\n",
    "longest_title = df_longest_title['title'].loc[index_longest_title]\n",
    "\n",
    "print('The longest \"complete\" title among all books is:\"' + longest_title+ '\"' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ4.4: What is the shortest overall book title in the dataset? If you find something strange, provide a comment on what happened and an alternative answer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHORTEST TITLE \n",
    "shortest_title_len = df_2.title_len.min()\n",
    "index_shortest_title = df_2.title_len.idxmin()\n",
    "shortest_title = df_2.title.loc[index_shortest_title]\n",
    "\n",
    "print(\"The shortest title along all the titles is \" + shortest_title + \" and its length is \" + str(shortest_title_len) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The found shortest title is one of a book that doesn't exist on GoodReads, so now we try to find the \"meaningful\" shortest title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting a title free from spaces, in order to drop all the empty & non-significant ones\n",
    "df_2['no_space_titles'] = df_2['title'].apply(lambda title: title.replace(\" \", \"\"))\n",
    "df_2['no_space_title_len'] = df_2.no_space_titles.apply(lambda row:len(row))\n",
    "\n",
    "# if we keep a lower threshold on the title's length, we get back books that don't exist on GoodReads\n",
    "df_2_meaningful = df_2[ df_2['no_space_title_len'] > 3] \n",
    "\n",
    "#looking for the \"meaningful\" shortest title\n",
    "index_shortest_title_meaningful = df_2_meaningful.no_space_title_len.idxmin()\n",
    "shortest_title_len_meaningful = df_2.title_len.loc[index_shortest_title_meaningful] #using df_2 in order to get back the correct values\n",
    "shortest_title_meaningful = df_2.title.loc[index_shortest_title_meaningful]\n",
    "\n",
    "#Displaying results\n",
    "print('The \"meaningful\" shortest title among all the titles is \"' + shortest_title_meaningful + '\" and its length is ' + str(shortest_title_len_meaningful) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns we don't need out of this RQ\n",
    "df_2.drop(['title_len', 'no_space_titles', 'no_space_title_len'] , axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2bd956",
   "metadata": {},
   "source": [
    "# RQ5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa85818f",
   "metadata": {},
   "source": [
    "## RQ5.1 Plot the top 10 most influential authors regarding their fan count and number of books. Who is the most influential author?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261d1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rq5_1 = df_1[['name', 'works_count', 'fans_count', 'gender']]\n",
    "df_rq5_1 = df_rq5_1[df_rq5_1.name != 'Wikipedia']\n",
    "df_rq5_1_grouped = df_rq5_1.groupby('name')[[ 'fans_count']].sum()\n",
    "df_rq5_1_grouped = df_rq5_1_grouped.reset_index()\n",
    "df_rq5_grouped_sorted = df_rq5_1_grouped.sort_values(by=['fans_count'], ascending = False)\n",
    "df_rq5_grouped_sorted.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most influential author seems to be Stephen King with 766.035 as fans_count, that nearly doubles the second, Veronica Roth who has 455.358 as fans_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d46dec",
   "metadata": {},
   "source": [
    "## RQ5.2 Have they published any series of books? If any, extract the longest series name among these authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6332726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "influential_authors = ['Stephen King', 'Veronica Roth', 'Bill Gates', 'Colleen Hoover',\t'Rick Riordan', 'James Patterson', 'John Green',\n",
    "                        'Neil deGrasse Tyson', 'Cassandra Clare', 'Mindy Kaling']\n",
    "\n",
    "df_influential_authors = df_2[df_2['author_name'].isin(influential_authors)]\n",
    "df_influential_authors = df_influential_authors[df_influential_authors.series_name != '']\n",
    "unique_series_names = df_influential_authors['series_name'].unique()\n",
    "for series_name in unique_series_names:\n",
    "    print(series_name)\n",
    "longest_series_name = max(unique_series_names, key=len)\n",
    "\n",
    "print(\"The longest series name is:\", longest_series_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89de8d25",
   "metadata": {},
   "source": [
    "## RQ5.3 How many of these authors have been published in different formats? Provide a meaningful chart on the distribution of the formats and comment on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ad60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "influential_authors = ['Stephen King', 'Veronica Roth', 'Bill Gates', 'Colleen Hoover',\t'Rick Riordan', 'James Patterson', 'John Green',\n",
    "                        'Neil deGrasse Tyson', 'Cassandra Clare', 'Mindy Kaling']\n",
    "\n",
    "df_influential_authors = df_2[df_2['author_name'].isin(influential_authors)]\n",
    "df_influential_authors = df_influential_authors[df_influential_authors.format != '']\n",
    "author_format_counts = df_influential_authors.groupby('author_name')['format'].nunique()\n",
    "authors_with_multiple_formats = author_format_counts[author_format_counts > 1]\n",
    "print(\"Authors who have published in more than one format:\")\n",
    "print(authors_with_multiple_formats)\n",
    "format_counts = df_influential_authors['format'].value_counts()\n",
    "minimum_percentage = 2\n",
    "different_formats = format_counts[format_counts / format_counts.sum() * 100 >= minimum_percentage]\n",
    "\n",
    "if not different_formats.empty:\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie(different_formats, labels=different_formats.index, autopct='%1.1f%%')\n",
    "    plt.title(\"Distribution of Book Formats\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No formats meet the specified threshold for inclusion in the pie chart.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d35fbd",
   "metadata": {},
   "source": [
    "## RQ5.4 Provide information about the general response from readers (number of fans, average rating, number of reviews, etc.), divide the authors by gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcfd096",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the infromation we want to have for each author are: the number of fans, works-count, ratings_count, of average_ratings, text_reviews_count\n",
    "df_rq5_4 = df_1\n",
    "new_df = df_rq5_4.groupby('name').agg({\n",
    "    'fans_count': 'sum',\n",
    "    'works_count': 'sum',\n",
    "    'ratings_count': 'sum',\n",
    "    'average_rating': 'mean',\n",
    "    'text_reviews_count': 'sum'\n",
    "}).reset_index()\n",
    "new_df = new_df.sort_values(by = 'fans_count', ascending = False)\n",
    "new_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d72180",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_count = df_1['gender'].value_counts()\n",
    "print(gender_count.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05542fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_gender = ['nonbinary', 'genderqueer', 'agender', 'genderfluid', 'gender fluid', 'agender', 'neutral', 'transgender']\n",
    "df_rq5_4['gender'] = df_rq5_4['gender'].str.strip()\n",
    "df_rq5_4['gender'] = df_rq5_4['gender'].str.lower()\n",
    "df_rq5_4['gender'] = df_rq5_4['gender'].str.replace('-', '')\n",
    "df_rq5_4 = df_rq5_4[df_rq5_4.gender != '']\n",
    "\n",
    "def categorize_gender(x):\n",
    "    if 'trans' in x:\n",
    "        return 'other'\n",
    "    if 'fluid' in x:\n",
    "        return 'other'\n",
    "    if 'they' in x:\n",
    "        return 'other'\n",
    "    if x in other_gender:\n",
    "        return 'other'\n",
    "    else:\n",
    "        return x\n",
    "df_rq5_4['gender'] = df_rq5_4['gender'].apply(categorize_gender)\n",
    "df_rq5_4 = df_rq5_4[df_rq5_4['gender'].isin(['male','female','other'])]\n",
    "a = df_rq5_4['gender'].value_counts()\n",
    "print(a)\n",
    "\n",
    "gender_counts = df_rq5_4['gender'].value_counts()\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(gender_counts, labels=gender_counts.index, autopct='%1.1f%%', startangle=140)\n",
    "plt.title('Gender Distribution of authors')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the data under the column 'gender' of df_1, we noticed that many genders, different from 'male' and 'female', had strange values, not linked with the genders different from the mian 2. After this discovery we decided to create a new category called 'other' where real genders, that are not male and female could be placed in.\n",
    "Analyzing the final pie chart about the distribution of genders, we can clearly see how the main 2 genders are the most diffused among the book's authors; at the same time, we would like to underline the not complete absence of other genders, that could be seen as an intial step for a future increase of the number of authors that identify in one of the categories inside 'other'.\n",
    "\n",
    "Talking about structural biases, they can manifest themselves in various ways, but in this case the overrepresentation of male authors over female authors seems absent; in fact, the 2 percentages are not so far from each other and this doesn't create unbalancement.\n",
    "Having an umbalanced dataset, in this case, could limit the diversity of voices and perspectives in literature, leading to a narrower range of experiences and viewpoints in the books that are published. It's important to consider the historical and social factors that contribute to this topic. Recognizing and addressing structural bias in literature and authorship is crucial for promoting diversity and inclusivity, is essential to create an environment where authors of all genders have equal opportunities to share their stories and experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ae4ef",
   "metadata": {},
   "source": [
    "# RQ6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4936d",
   "metadata": {},
   "source": [
    "## RQ6.1 Provide the average time gap between two subsequent publications for a series of books and those not belonging to a series. What do you expect to see, and what is the actual answer to this question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299dafa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to split the books that belong to a series and the books that doesn't belong to a series, we assume that the books with empty values in the columns 'series_name' and 'series_id', don't belong to a series, so are single books\n",
    "book_series = df_influential_authors[df_influential_authors['series_name'] != '']\n",
    "book_series = df_influential_authors[df_influential_authors['series_id'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d272404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we drop all the cells where there isn't the publication date because if there isn't we can't claculate the difference: publication_date - original_publication_date = time gap\n",
    "bs_without_missing = book_series[book_series['publication_date'] != '']\n",
    "bs_without_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e860b0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_without_missing['publication_date'] = pd.to_datetime(bs_without_missing['publication_date'], format='%Y', errors='coerce')\n",
    "bs_without_missing['original_publication_date'] = pd.to_datetime(bs_without_missing['original_publication_date'], format='%Y', errors='coerce')\n",
    "bs_without_missing['publication_difference_years'] = bs_without_missing['publication_date'].dt.year - bs_without_missing['original_publication_date'].dt.year\n",
    "mask = (bs_without_missing['publication_date'].dt.year <= 2023) | (bs_without_missing['publication_date'].isnull())\n",
    "bs_without_missing = bs_without_missing.loc[mask].dropna()\n",
    "average_difference = bs_without_missing['publication_difference_years'].mean()\n",
    "print(\"Average Difference in Years:\", average_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7187c6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_without_series = df_influential_authors[df_influential_authors['series_name'] == '']\n",
    "book_without_series = df_influential_authors[df_influential_authors['series_id'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09142d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsws_without_missing = book_without_series[book_without_series['publication_date'] != '']\n",
    "bsws_without_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b3cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsws_without_missing['publication_date'] = pd.to_datetime(bsws_without_missing['publication_date'], format='%Y', errors='coerce')\n",
    "bsws_without_missing['original_publication_date'] = pd.to_datetime(bsws_without_missing['original_publication_date'], format='%Y', errors='coerce')\n",
    "bsws_without_missing['publication_difference_years'] = bsws_without_missing['publication_date'].dt.year - bsws_without_missing['original_publication_date'].dt.year\n",
    "mask = (bsws_without_missing['publication_date'].dt.year <= 2023) | (bsws_without_missing['publication_date'].isnull())\n",
    "bsws_without_missing = bsws_without_missing.loc[mask].dropna()\n",
    "bsws_average_difference = bsws_without_missing['publication_difference_years'].mean()\n",
    "print(\"Average Difference in Years:\", bsws_average_difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before plotting the results the assumpion we made was that the gap should have been more or less the same, basing our hypothesis on the fact that what really influences the authors' productivity isn't the belonging of a book to a seires or not, but other factors such as: the time management, inspiration and creativity, personal experiences, motivation or the emotional state. Related to the previous statement, we would like to underline how the market logics, where the publishers could stress an author to realease a new book consistenly, isn't really related to a book belonging to a series or not, but is related to how the market percieves that author, how many fans she/he has and what is the degree of influence on the market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf64fb3",
   "metadata": {},
   "source": [
    "## RQ6.2 For each of the authors, give a convenient plot showing how many books has the given author published UP TO a given year. Are these authors contemporary with each other? Can you notice a range of years where their production rate was higher?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236938c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_influential_authors['publication_date'] = pd.to_datetime(df_influential_authors['publication_date'], format='%Y', errors='coerce')\n",
    "df_influential_authors['publication_year'] = df_influential_authors['publication_date'].dt.year\n",
    "author_year_counts = df_influential_authors.groupby(['author_name', 'publication_year']).size().reset_index(name='work_count')\n",
    "author_year_counts['cumulative_books'] = author_year_counts.groupby('author_name')['work_count'].cumsum()\n",
    "target_year = 2022\n",
    "data_for_target_year = author_year_counts[author_year_counts['publication_year'] <= target_year]\n",
    "\n",
    "for author, data in data_for_target_year.groupby('author_name'):\n",
    "    plt.plot(data['publication_year'], data['cumulative_books'], label=author)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Cumulative Number of Books Published')\n",
    "plt.yscale(\"log\")\n",
    "plt.title(f'Books Published by Authors up to {target_year}')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots shows how these authors are conteporary, with just Stephen King and James Patterson having the first books published between the 70s and the 80s. All the other authors started to published their books in the first years of the 2000s, in fact, the lines of the overlap around that years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ7 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ7.1: Estimate the probability that a book has over 30% of the ratings above 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organizing data\n",
    "df_ratings = df_2[['work_id', 'rating_dist', 'ratings_count']]\n",
    "\n",
    "#initilizing the number of ratings over 4 (which are the 5 stars ratings) overwriting on rating_dist\n",
    "df_ratings.loc[:,'rating_dist'] = df_ratings['rating_dist'].apply(lambda row: row[4] + row[5])\n",
    "\n",
    "#dividing books between having or not 30% of ratings above 4\n",
    "df_ratings.loc[:, '30_over_4'] = df_ratings.apply(lambda row: bool( row['rating_dist']/row['ratings_count'] > 0.3), axis= 1)\n",
    "\n",
    "#probability estimation\n",
    "books_30over4 = sum(df_ratings['30_over_4'])\n",
    "books_total = len(df_ratings)\n",
    "\n",
    "probability_30over4 = books_30over4/books_total\n",
    "\n",
    "#Showing results\n",
    "print(\"The probability that a book has over 30 percent of ratings over 4 is:\", round(probability_30over4, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ7.2: Estimate the probability that an author publishes a new book within two years from its last work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting relevant data\n",
    "df_publications = df_2[['author_id', 'work_id', 'original_publication_date']]\n",
    "df_publications.drop_duplicates(subset='work_id', inplace=True)\n",
    "\n",
    "#pre-processing 'publication_date' column \n",
    "pattern = r'(^[0-9]{4}-[0-9]{2}-[0-9]{1,2}$)|(^[0-9]{4}$)|(^[0-9]{4}-[0-9]{1,2}$)'\n",
    "df_publications= df_publications[df_publications['original_publication_date'].apply(lambda row: bool(re.search(pattern, row)))]\n",
    "df_publications['original_publication_date'] = df_publications['original_publication_date'].apply(lambda row: re.split('-', row)) #this way we obtain year=df_rq3['original_publication_date'][0], month=df_rq3['original_publication_date'][1]\n",
    "df_publications['original_publication_year'] = df_publications['original_publication_date'].apply(lambda row: int(row[0]))\n",
    "\n",
    "#grouping\n",
    "df_publications = df_publications.groupby('author_id').agg(list)\n",
    "\n",
    "#getting the number of authors that published books in less than two years apart\n",
    "df_publications['original_publication_year'] = df_publications.original_publication_year.apply(lambda row: sorted(row)[::-1] )  #sorting publication years in descending order\n",
    "df_publications['years_diff'] = df_publications.original_publication_year.apply(lambda row: [ ( (row[i] - row[i+1]) <= 2 ) for i in range(len(row)-1)])\n",
    "closerthan2years=[1 for row in df_publications['years_diff'] if sum(row)>0]\n",
    "\n",
    "#estimating probability\n",
    "probability_closer_than2years = sum(closerthan2years)/len(df_publications)\n",
    "print(\"The probability that an author publishes a new book  within two years from its last work is:\", round(probability_closer_than2years, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ7.3: In the file list.json, you will find a peculiar list named \"The Worst Books of All Time.\" Estimate the probability of a book being included in this list, knowing it has more than 700 pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "#to load lists.json, set to_be_loaded == True\n",
    "to_be_loaded = True\n",
    "\n",
    "if to_be_loaded == True:\n",
    "    lists = pd.read_json('list.json', lines=True)\n",
    "\n",
    "#first step: getting needed data\n",
    "    # index of \"The worst books of all time\" == 0\n",
    "worst_of_all_time = lists.books.iloc[0]    #it's a list\n",
    "total_books_worst = int(lists.num_books.iloc[0])\n",
    "\n",
    "\n",
    "#getting a list of book_keys of the books in \"The worst books of all time\"\n",
    "ids = []\n",
    "for book in worst_of_all_time: \n",
    "    ids.append(int(book['book_id']))\n",
    "\n",
    "#number of books with more than 700 pages in \"The worst books of all time\"\n",
    "num_pages_list = df_2['num_pages'][df_2['id'].isin(ids)]\n",
    "more_than_700_worst = sum([ (pages>700) for pages in num_pages_list ])\n",
    "\n",
    "#number of books with more than 700 pages in all the dataset\n",
    "more_than_700_all = len(df_2[df_2['num_pages'] > 700]) \n",
    "\n",
    "#conditional probability \n",
    "probability_worst_over700 = more_than_700_worst / more_than_700_all\n",
    "\n",
    "#show results\n",
    "print('The probability that a book is in \"The worst books of all time\" knowing that it has more than 700 pages is:', round(probability_worst_over700, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ7.4: Are the events X=’Being Included in The Worst Books of All Time list’ and Y=’Having more than 700 pages’ independent? Explain how you have obtained your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check if the events *X=’Being Included in The Worst Books of All Time list’* and *Y=’Having more than 700 pages’* are independent by comparing *Pr(X | Y)* obtained in the prevous step with *P(X)*. Indeed, we know that these two values are the same iff the two events are independent. \\\n",
    "We keep track of the possible numeric error raised by the use of a computer using *math.isclose* instead of *==*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARE X = ’Being Included in The Worst Books of All Time list’ AND Y =’Having more than 700 pages’ INDEPENDENT? \n",
    "\n",
    "probability_inworst = total_books_worst/len(df_2)\n",
    "if math.isclose(probability_worst_over700, probability_inworst):\n",
    "    print(\"The two evwnts are independent.\")\n",
    "else:\n",
    "    print(\"The two evwnts aren't independent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RQ 8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ8.1: Can you demonstrate that readers usually rate the longest books as the worst?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1st procedure: correlation coefficient\n",
    "\n",
    "df_2[['average_rating', 'num_pages']][(df_2['average_rating']>=1)].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2nd procedure: linear regression + visualization\n",
    "\n",
    "''' X = number of pages\n",
    "    Y = average rating '''\n",
    "\n",
    "X = df_2.num_pages[(df_2['average_rating']>=1)]\n",
    "Y = df_2.average_rating[(df_2['average_rating']>=1)]\n",
    "Z=X[X<20000]\n",
    "\n",
    "plt.scatter(X[(Y>0) & (X<20000)], Y[(Y>0) & (X<20000)], 1)\n",
    "\n",
    "X = sm.add_constant(X, prepend=True, has_constant='skip')   #adding constant to get the intercept (not given in default mode in ols)\n",
    "\n",
    "regression_model = sm.OLS(Y, X)\n",
    "regression_results = regression_model.fit()\n",
    "\n",
    "b1 = regression_results.params['num_pages']\n",
    "b0 = regression_results.params['const']\n",
    "print(\"Regression coefficient:\", b1)\n",
    "\n",
    "\n",
    "plt.plot(Z, b0 + b1*Z, color= 'red') # Y = b0 + b1*X is the equation of the \"regession line\"\n",
    "\n",
    "plt.xlabel('Number of pages')\n",
    "plt.ylabel('Average Rating')\n",
    "\n",
    "plt.title('Regression plot over scatterplot of the dataset')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, we chose to consider only the books with average rating greater or equal to $1$ because the ratings' distribution takes values from 1 to 5, so those books are the ones which don't have any ratings or have misrecorded avarege.\n",
    "\n",
    "To address this question, we followed two methodologies: the first, and faster, was to look at the correlation coefficient between the books' avarege rate and number of pages. \n",
    "As showed above, the correlation coefficient we got is reaaly close to zero, meaning that there is no relevant correlation between the variables we took into exam. \n",
    "Moreover, we choose to also perform a linear regression in order to corroborate our results. This procedure is also useful to get a visual representation of our analysis.\n",
    "The risults of this second procedure confirmed the ones given from the correlation coefficient. Indeed, we got a regression coefficient proportional to $10^{-10}$, which is practically zero, meaning that the two variables are orthogonal and not corelated. \n",
    "On a sidenote, to make the plot of our results actually readable, we choose to plot the books with less than $20000$  pages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ8.2: Compare the average rate distribution for English and non-English books with a proper statistical procedure. What can you conclude about those two groups?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting needed data\n",
    "english_books = df_2[df_2['language'].isin(['eng', 'en-US', 'en-GB', 'US'])]['average_rating']\n",
    "non_english_books = df_2[~df_2['language'].isin(['eng', 'en-US', 'en-GB', 'US',  ''])]['average_rating'] #excluding also the books that don't have the language registered, cause they could be misclassified if keeped\n",
    "\n",
    "#excluding books with average rating <1 because misregistered\n",
    "english_books = english_books[english_books >= 1 ]\n",
    "non_english_books = non_english_books[non_english_books >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbins=50\n",
    "\n",
    "#english book average rate\n",
    "plt.subplot(1, 2, 1)\n",
    "hist_en, bins_en, _ = plt.hist(english_books, bins=numbins, density=True, alpha=0.5, color='b', edgecolor='k')\n",
    "\n",
    "# highlighting mean and median\n",
    "mean_en = np.mean(english_books)\n",
    "median_en = np.median(english_books)\n",
    "plt.axvline(mean_en, color='purple', linestyle='dotted', linewidth=2, label='Mean')\n",
    "plt.axvline(median_en, color='r', linestyle='dotted', linewidth=2, label='Median')\n",
    "\n",
    "\n",
    "plt.grid(axis= 'y', linestyle='dotted')\n",
    "plt.xlabel('Average rate')\n",
    "plt.title(\"English books\")\n",
    "plt.legend()\n",
    "\n",
    "#non english books average rate\n",
    "plt.subplot(1,2,2)\n",
    "hist_non_en, bins_non_en, _ = plt.hist(non_english_books, bins=numbins, density=True, alpha=0.5, color='b', edgecolor='k')\n",
    "\n",
    "# highlighting mean and median\n",
    "mean_nonen = np.mean(non_english_books)\n",
    "median_nonen = np.median(non_english_books)\n",
    "\n",
    "plt.axvline(mean_nonen, color='purple', linestyle='dotted', linewidth=2, label='Mean')\n",
    "plt.axvline(median_nonen, color='r', linestyle='dotted', linewidth=2, label='Median')\n",
    "\n",
    "plt.grid(axis= 'y', linestyle='dotted')\n",
    "plt.xlabel('Average rate')\n",
    "plt.title(\"Non-english books\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a first visual exploration of the two groups of the dataset we don't notice any valuable difference between them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descriptions\n",
    "print(\"English books statistics: \\n\") \n",
    "english_books.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non-english books statistics: \\n\")\n",
    "non_english_books.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the meaningful statistics plotted above we notice that the English-books group is larger than the other one, but their relevant statistics aren't that different. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OPTION 1: t-Test**\n",
    "\n",
    "Warning: we know that when performing a t-Test the data distribution is required to be Normal. We tested  the samples for that using the pre-implemented Shapiro test (using *stats.shapiro(data)* ), but we got a warning back saying the following: \"*UserWarning: p-value may not be accurate for N > 5000.*\"\n",
    "\n",
    "On the other hand, we know that for i.i.d. r.v., their mean is asintotically Normal for the Central Limit Theorem. The problem is that in this case, the samples may not be identically distributed: euristically, the ratings of a book are strictly book dependet, and so is their mean. For this reasons, we choose to go for a non parameric Kolmogorov-Shmirov test, which isn't distribution-dependent.\n",
    "\n",
    "**OPTION 2: Kolmogorov-Shmirov test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The null hypothesis is that the two distributions are identical, F(x)=G(x) for all x;\n",
    "from scipy import stats\n",
    "ks_stat, p_value = stats.ks_2samp(english_books, non_english_books)\n",
    "\n",
    "# Results \n",
    "alpha = 0.05  #level of significance\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(f\"The two samples are statistically different (p = {p_value})\")\n",
    "else:\n",
    "    print(f\"The two samples are not statistically different (p = {p_value})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the samples are different, we check which one is dominant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative='greater': The null hypothesis is that F(x) <= G(x) for all x\n",
    "\n",
    "ks_stat_great, p_value_great = stats.ks_2samp(english_books, non_english_books, alternative='greater')\n",
    "\n",
    "# Results \n",
    "if p_value_great< alpha:\n",
    "    print(f\"The distribution of english books is stocastically dominant (p = {p_value_great})\")\n",
    "else:\n",
    "    print(f\"The distribution of non-english books is stocastically dominant  (p = {p_value_great})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot to confront the distributions\n",
    "\n",
    "datarange= max(english_books) - min(english_books)\n",
    "\n",
    "bin_centers_en = bins_en+ (0.5*datarange/numbins)           #numbins, bincenters took from the previous plot(histograms)\n",
    "bin_centers_non_en = bins_non_en+ (0.5*datarange/numbins)\n",
    "\n",
    "\n",
    "plt.subplot(1,1,1)\n",
    "plt.plot(bin_centers_en[:-1], hist_en, 'r-', linewidth=1, label='English')\n",
    "plt.plot(bin_centers_non_en[:-1], hist_non_en, 'g-', linewidth=1, label='Non-English')\n",
    "\n",
    "plt.title(\"English and Non-English books' distributions\")\n",
    "plt.xlabel('Average Rate')\n",
    "plt.ylabel('')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ8.3: About the two groups in the previous question, extract helpful statistics like mode, mean, median, and quartiles, explaining their role in a box plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.boxplot(english_books, meanline=True, showmeans=True)\n",
    "plt.grid(True, linestyle='dotted')\n",
    "plt.title(\"English books\")\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.boxplot(non_english_books, meanline=True, showmeans=True)\n",
    "plt.grid(True, linestyle='dotted')\n",
    "plt.title(\"Non-english books\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "print(\"Some meaningful statistics about english books' average rate are: \\n\", non_english_books.describe())\n",
    "print(\"Some meaningful statistics about non-english books' average rate are: \\n\", non_english_books.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ8.4: It seems reasonable to assume that authors with more fans should have more reviews, but maybe their fans are a bit lazy. Confirm or reject this with a convenient statistical test or a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform this task, we decided to use the informations in *\"ratings_count\"* and not the ones in *\"text_reviews_count\"* because we interpreted also the ratings as a review of the book, in a broader sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression to estimate the number of reviews based on fan numbers and seeing if the \"regression line\" (come si chiama?) has a positive coefficient \n",
    "\n",
    "#X = fans count;  Y =  ratings count.\n",
    "# Y = b0 + b1 X\n",
    "\n",
    "X = df_1.fans_count\n",
    "Y = df_1.ratings_count\n",
    "X = sm.add_constant(X, prepend=True, has_constant='skip')   #adding constant to get the intercept (not given in default mode in ols)\n",
    "\n",
    "regression_model = sm.OLS(Y, X)\n",
    "regression_results = regression_model.fit()\n",
    "\n",
    "#getting regression coefficients\n",
    "b1 = regression_results.params['fans_count']\n",
    "b0 = regression_results.params['const']\n",
    "\n",
    "print(\"Regression coefficient b1:\", b1)\n",
    "print(\"Regression coefficient b0 (intercept):\", b0)\n",
    "\n",
    "#F test\n",
    "f_statistic = regression_results.fvalue\n",
    "p_value = regression_results.f_pvalue\n",
    "print(\"F-statistic:\", f_statistic)\n",
    "print(\"F - test p-value:\", p_value)\n",
    "\n",
    "#rsquared\n",
    "print('R-squared:', regression_results.rsquared) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot regression vs points \n",
    "\n",
    "X = df_1.fans_count\n",
    "Y = df_1.ratings_count\n",
    "\n",
    "plt.scatter(X, Y, 1)\n",
    "plt.plot(X, b0 + b1*X, color= 'red') #b0 + b1*X is the ewuation of the \"regession line\"\n",
    "\n",
    "plt.xlabel('Fans count')\n",
    "plt.ylabel('Ratings count')\n",
    "\n",
    "plt.title('Regression plot over scatterplot of the dataset')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, we decided to perform a linear regression procedure against the two analized variables: \"fans_count\" and \"ratings_count\".\n",
    "\n",
    "The results show that the model isn't actually very explicaytive (we have a low Rsquared coefficient), but on the other hand the assumption it's true for the well-represented part of the data. \n",
    "On the other hand, the F-test results tell us that our model has a better fit than the one with only the intercept.\n",
    "In the end, we have that the regression coefficient we obtained is positive, meaning that there's a direct proportionality relationship between the two analized variables, so the assumption made in this case (the fact that the authors with more fans should haeìve more ratings) is true.\n",
    "\n",
    "To corroborate this affermation, we decided to also calculate the correlation coefficient between the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1[['fans_count', 'ratings_count']].corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation coefficient we got is approximatly $0.6$, meaning that the two variables aren't orthogonal/independent, but indeed there's a positive correlation btween them, which leads to a type of relationship like \"when one grows, so does the other\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**8.5: Provide a short survey about helpful statistical tests in data analysis and mining: focus on hypothesis design and the difference between parametric and nonparametric tests, explaining the reasons behind the choice of one of these two tests.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical tests are useful to get information about data distribution and other statistical attributes. In particoular, they could be used to confirm or reject assumption about data. \n",
    "In both cases, the data must themselves satisfy assumptions in order for the tests to be correctly applicable. \n",
    "In the case of parametric tests, normality of the data (i.e. that their distribution is normal), assumptions about the variance of the data, and independence of the observations are often required. \n",
    "Obviously, these characteristics constitute a discriminating factor in the choice of test to be applied, along with the type of analysis that is required.\n",
    "In the event that the data do not verify the required assumptions on the distribution, but only that of independence of the observations, it is plausible that a non-parametric test is applicable, which is independent of the distribution of the data.\n",
    "\n",
    "Hypothesis tests can also be divided into bilateral and unilateral tests. This distinction refers to the type of hypotheses that are tested, i.e. how these hypotheses are designed. In the case where, under the assumption that the null hypothesis is one of equality (between two parameters in question), the alternative hypothesis is one of inequality ($\\neq$), we speak of bilateral tests, whereas in the case where the alternative is one of strict inequality ( $>$, $<$) we speak of unilateral tests. The difference refers to the tails of the distribution that are considered in the analysis.\n",
    "\n",
    "Some useful tests in this type of analysis are: \n",
    "- t-test: parametric test used to check whether the mean value of a distribution deviates significantly from a fixed value or not. The test is based on Student's t-distribution, requires the assumption of normality and is based on the following statistic: \n",
    "\\begin{equation*}\n",
    "Y_{0}={\\frac {\\bar{X} -\\mu _{0}}{s/{ \\sqrt{n}}}}\n",
    "\\end{equation*}\n",
    "Knowledge of the variance of the distribution is not required.\n",
    "\n",
    "- F-test: is a parametric test to verify that two normal distributions have the same variance. It is based on Fisher's distribution and requires sample independence. The reference statistic for such a test is: \n",
    "\\begin{equation*}\n",
    "F={\\frac {S_{X}^{2}}{S_{Y}^{2}}}\n",
    "\\end{equation*}, \n",
    "where $S_{X}$ and $S_{Y}$ are estimators of the variance of the two distributions $X$ and $Y$.\n",
    "It is verified that $F$ follows the Fisher distribution under the assumption $H_{0}=(\\sigma _{X}^{2}=\\sigma _{Y}^{2})$, hence the name of the test.\n",
    "\n",
    "- Kolmogorov-Smirnov test: is a non-parametric test aimed at comparing either the distribution of a sample with a known distribution, or the distributions of two samples. It requires the independence of the samples in the two-sample case; in the two-sample case, their empirical distribution is used. The assumptions tested in the two-sample case are as follows: \n",
    "\\begin{align*}\n",
    "& H_{0}: {F(x)=F_{0}(x), \\forall x} \\\\\n",
    "& H_{1}: {\\exists x : F(x)\\neq F_{0}(x)}\n",
    "\\end{align*}\n",
    "With $F$, $F_0$ the two distributions under consideration. The test statistic is the sup distance between the two (possibly empirical) distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Points - Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points 1.1: Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload authors.json dataset, and filter authors with at least 100 reviews. Do the same using Pandas and compare performance in terms of milliseconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first installed PySpark by:\n",
    "$ pip install pyspark\n",
    "\n",
    "- Then, used 'SparkSession.builder.appName(\"FilterAuthors\").getOrCreate()' to create a Spark session in order to use its built-in libraries and methods.\n",
    "- I read the json file with '.read.json()' method. PySpark is basically designed to work with Big data.\n",
    "- Pyspark uses resilient distributed datasets (RDDs) to work parallel on the data. Hence, it performs better than pandas.\n",
    "- PySpark uses lazy processing, retrieving data from disk only when required, while the pandas module stores all data in memory, resulting in higher memory consumption compared to PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------------+------------------+\n",
      "|     id|           name|text_reviews_count|\n",
      "+-------+---------------+------------------+\n",
      "|   3389|   Stephen King|            608956|\n",
      "|1077326|   J.K. Rowling|            606373|\n",
      "| 153394|Suzanne Collins|            427224|\n",
      "| 150038|Cassandra Clare|            416177|\n",
      "|3433047|  Sarah J. Maas|            372923|\n",
      "+-------+---------------+------------------+\n",
      "\n",
      "CPU times: user 38.1 ms, sys: 11.3 ms, total: 49.5 ms\n",
      "Wall time: 2.61 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"FilterAuthors\").getOrCreate()\n",
    "\n",
    "# Read the JSON file\n",
    "df = spark.read.json(\"lighter_authors.json\")\n",
    "\n",
    "# Filter the DataFrame based on the text_reviews_count property\n",
    "filtered_df = df.filter(col(\"text_reviews_count\") >= 100)\n",
    "\n",
    "# Sort the DataFrame in descending order by 'text_reviews_count'\n",
    "sorted_df = filtered_df.sort(col(\"text_reviews_count\").desc())\n",
    "\n",
    "# Select specific columns and return the first 5 rows\n",
    "result_df = sorted_df.select(\"id\", \"name\", \"text_reviews_count\").limit(5)\n",
    "\n",
    "# Show the resulting DataFrame\n",
    "result_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             id             name  text_reviews_count\n",
      "1017       3389     Stephen King              608956\n",
      "86500   1077326     J.K. Rowling              606373\n",
      "27522    153394  Suzanne Collins              427224\n",
      "27110    150038  Cassandra Clare              416177\n",
      "157593  3433047    Sarah J. Maas              372923\n",
      "CPU times: user 23.2 s, sys: 1.93 s, total: 25.1 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "\n",
    "authors_json = pd.read_json('./lighter_authors.json', lines = True, chunksize=100)\n",
    "\n",
    "authors_df = pd.concat(authors_json, ignore_index=False)\n",
    "\n",
    "# Filter the DataFrame based on 'text_reviews_count'\n",
    "filtered_df = authors_df[authors_df['text_reviews_count'] >= 100]\n",
    "\n",
    "# Sort the filtered DataFrame in descending order by 'text_reviews_count'\n",
    "sorted_df = filtered_df.sort_values(by='text_reviews_count', ascending=False)\n",
    "\n",
    "# Select specific columns and return the first 5 rows\n",
    "result_df = sorted_df[['id', 'name', 'text_reviews_count']].head(5)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the performance of PySpark Vs Pandas:\n",
    "\n",
    "| Library     | CPU user    | sys         | total       | Wall time   | \n",
    "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
    "| PySpark | 38.1 ms | 11.3 ms | 49.5 | 2.61 |\n",
    "| Pandas | 23.2 s | 1.93  | 25.1 s | 25.1 s |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points 1.2: Select one alternative library to Pandas (i.e., Dask, Polar, Vaex, Datatable, etc.), upload books.json, and join them with authors.json based on author_id. How many books don’t have a match for the author?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"JoinBooksAndAuthors\").getOrCreate()\n",
    "\n",
    "# Read the 'lighter_books.json' and 'lighter_authors.json' files\n",
    "books_df = spark.read.json(\"lighter_books.json\")\n",
    "authors_df = spark.read.json(\"lighter_authors.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in 'lighter_books.json': ['asin', 'author_id', 'author_name', 'authors', 'average_rating', 'description', 'edition_information', 'format', 'id', 'image_url', 'isbn', 'isbn13', 'language', 'num_pages', 'original_publication_date', 'publication_date', 'publisher', 'rating_dist', 'ratings_count', 'series_id', 'series_name', 'series_position', 'shelves', 'text_reviews_count', 'title', 'work_id']\n",
      "------------------------------------------------------------\n",
      "Columns in 'lighter_authors.json': ['about', 'average_rating', 'book_ids', 'fans_count', 'gender', 'id', 'image_url', 'name', 'ratings_count', 'text_reviews_count', 'work_ids', 'works_count']\n"
     ]
    }
   ],
   "source": [
    "# Get the column names for each DataFrame for better understanding of the structure of datasets.\n",
    "books_columns = books_df.columns\n",
    "authors_columns = authors_df.columns\n",
    "\n",
    "# Print the column names\n",
    "print(\"Columns in 'lighter_books.json':\", books_columns)\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(\"Columns in 'lighter_authors.json':\", authors_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of books without a matching author: 0\n"
     ]
    }
   ],
   "source": [
    "# Perform the join operation. books with the same author_id as authors with the same id, will be joined together.   \n",
    "joined_df = books_df.join(authors_df, books_df['author_id'] == authors_df['id'], 'left_outer')\n",
    "\n",
    "# Count the number of books without a matching author\n",
    "books_without_author = joined_df.filter(authors_df['id'].isNull()).count()\n",
    "\n",
    "# Count the number of books that couldn't be joined\n",
    "unjoined_count = joined_df.filter(authors_df['id'].isNull()).count()\n",
    "\n",
    "# Show the count\n",
    "print(f\"Number of books that could not be joined: {unjoined_count}\")\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second approach\n",
    "\n",
    "In this approach, \n",
    "- first, we return the number of rows in lighter_books.json. which is stored in 'num_rows_books'.\n",
    "- Then, we define a variable named 'joined_df_count' as an incrementation. if any row of lighter_books.json and any row of lighter_authors.json were be joint, increase 'joined_df_count' for 1. or basically calculate the count of joined_df.\n",
    "- Then, we return the difference between the 'joined_df_count' and number of rows from the books.json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/10/31 12:26:03 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Rows in lighter_books.json: 7027431\n",
      "Value of 'joined_df_count': 7027431\n",
      "Difference: 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"JoinAndCount\").getOrCreate()\n",
    "\n",
    "# Read 'lighter_books.json' and 'lighter_authors.json'\n",
    "books_df = spark.read.json(\"lighter_books.json\")\n",
    "authors_df = spark.read.json(\"lighter_authors.json\")\n",
    "\n",
    "# Join the two DataFrames based on 'author_id' and 'id'\n",
    "joined_df = books_df.join(authors_df, books_df['author_id'] == authors_df['id'], 'inner')\n",
    "\n",
    "# Calculate the number of rows in 'lighter_books.json'\n",
    "num_rows_books = books_df.count()\n",
    "\n",
    "# Define and increment the variable 'k' for each matching row\n",
    "joined_df_count = joined_df.count()\n",
    "\n",
    "# Calculate the difference between 'k' and the number of rows in 'lighter_books.json'\n",
    "difference = joined_df_count - num_rows_books\n",
    "\n",
    "# Print the result\n",
    "print(\"Number of Rows in lighter_books.json:\", num_rows_books)\n",
    "print(\"Value of 'joined_df_count':\", joined_df_count)\n",
    "print(\"Difference:\", difference)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "As can be seen, all books have an id that exists in authors.json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Points 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points 2.1 & 2.2: \n",
    "## If you choose to text-mine books.json ’ descriptions, try to find a way to group books in genres using whatever procedure you want, highlighting words that are triggers for these choices.\n",
    "## If you choose to text-mine authors.json’ about-field, try to find a way to group authors in genres using whatever procedure you want, highlighting words that are triggers for these choices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+---------------+\n",
      "| id|                name|          genre|\n",
      "+---+--------------------+---------------+\n",
      "|  4|       Douglas Adams|        Romance|\n",
      "|  7|         Bill Bryson|    Non-Fiction|\n",
      "| 10|         Jude Fisher|        Fantasy|\n",
      "| 12|James Hamilton-Pa...|    Non-Fiction|\n",
      "| 14|         Mark Watson|         Comedy|\n",
      "| 16|       Edith Wharton|        Romance|\n",
      "| 17|       Luther Butler|        Romance|\n",
      "| 18|        Gary Paulsen|        Romance|\n",
      "| 20|           Dale Peck|       Children|\n",
      "| 23|       Angela Knight|        Romance|\n",
      "| 24|       Delia Sherman|        Fantasy|\n",
      "| 25|Patricia A. McKillip|        Romance|\n",
      "| 26|      Anne McCaffrey|Science Fiction|\n",
      "| 27|Zilpha Keatley Sn...|       Children|\n",
      "| 29|        Kate Horsley|        Romance|\n",
      "| 31|   Elaine Cunningham|        Fantasy|\n",
      "| 32|       Philippa Carr|     Historical|\n",
      "| 33|     Edward P. Jones|       Children|\n",
      "| 36|        Satyajit Das|           NULL|\n",
      "| 38|         Mark Smylie|        Fantasy|\n",
      "+---+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+--------------------+---------------+\n",
      "| id|               title|          genre|\n",
      "+---+--------------------+---------------+\n",
      "|  2|Harry Potter and ...|        Fantasy|\n",
      "|  3|Harry Potter and ...|        Romance|\n",
      "|  4|Harry Potter and ...|         Horror|\n",
      "|  5|Harry Potter and ...|        Fantasy|\n",
      "|  6|Harry Potter and ...|        Fantasy|\n",
      "|  7|The Harry Potter ...|        Mystery|\n",
      "|  8|Harry Potter Boxe...|           NULL|\n",
      "| 10|Harry Potter Coll...|        Mystery|\n",
      "| 11|The Hitchhiker's ...|Science Fiction|\n",
      "| 12|The Ultimate Hitc...|Science Fiction|\n",
      "| 13|The Ultimate Hitc...|Science Fiction|\n",
      "| 14|The Hitchhiker's ...|Science Fiction|\n",
      "| 15|The Hitchhiker's ...|      Adventure|\n",
      "| 16|The Hitchhiker's ...|Science Fiction|\n",
      "| 17|The Hitchhiker's ...|           NULL|\n",
      "| 18|The Ultimate Hitc...|Science Fiction|\n",
      "| 19|The Hitchhiker's ...|        Romance|\n",
      "| 21|A Short History o...|    Non-Fiction|\n",
      "| 22|Bill Bryson's Afr...|      Adventure|\n",
      "| 23|Bryson's Dictiona...|      Adventure|\n",
      "+---+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"GroupAuthorsIntoGenres\").getOrCreate()\n",
    "\n",
    "# Read 'lighter_authors.json' and 'lighter_books.json'\n",
    "authors_df = spark.read.json(\"lighter_authors.json\")\n",
    "books_df = spark.read.json(\"lighter_books.json\")\n",
    "\n",
    "# Define triggers for genres\n",
    "genre_triggers = {\n",
    "    \"Romance\": [\"love\", \"relationship\", \"heart\", \"passion\", \"kiss\", \"romantic\", \"affection\"],\n",
    "    \"Mystery\": [\"mystery\", \"detective\", \"crime\", \"suspense\", \"puzzle\", \"enigma\", \"whodunit\"],\n",
    "    \"Fantasy\": [\"fantasy\", \"magic\", \"kingdom\", \"mythical\", \"enchanted\", \"magical\"],\n",
    "    \"Science Fiction\": [\"science fiction\", \"space\", \"alien\", \"future\", \"technology\", \"extraterrestrial\"],\n",
    "    \"Non-Fiction\": [\"non-fiction\", \"memoir\", \"history\", \"biography\", \"documentary\", \"autobiography\"],\n",
    "    \"Adventure\": [\"adventure\", \"quest\", \"journey\", \"exploration\", \"excitement\", \"expedition\"],\n",
    "    \"Horror\": [\"horror\", \"scary\", \"fear\", \"terror\", \"supernatural\", \"haunting\"],\n",
    "    \"Comedy\": [\"comedy\", \"funny\", \"humor\", \"laughter\", \"comic\", \"hilarious\"],\n",
    "    \"Drama\": [\"drama\", \"tragedy\", \"emotional\", \"theatre\", \"intense\", \"performing\"],\n",
    "    \"Thriller\": [\"thriller\", \"suspenseful\", \"intense\", \"nail-biting\", \"tension\", \"exciting\"],\n",
    "    \"Biography\": [\"biography\", \"life story\", \"autobiography\", \"history\", \"memoir\", \"personal journey\"],\n",
    "    \"Fantasy\": [\"fantasy\", \"magic\", \"kingdom\", \"mythical\", \"enchanted\", \"magical\"],\n",
    "    \"Science Fiction\": [\"science fiction\", \"space\", \"alien\", \"future\", \"technology\", \"extraterrestrial\"],\n",
    "    \"Historical\": [\"historical\", \"period\", \"past\", \"epoch\", \"history\", \"retro\"],\n",
    "    \"Self-Help\": [\"self-help\", \"personal growth\", \"motivation\", \"positive\", \"self-improvement\", \"inspiration\"],\n",
    "    \"Cooking\": [\"cooking\", \"culinary\", \"recipes\", \"food\", \"chef\", \"cuisine\"],\n",
    "    \"Travel\": [\"travel\", \"adventure\", \"exploration\", \"journey\", \"destination\", \"vacation\"],\n",
    "    \"Science\": [\"science\", \"scientific\", \"discovery\", \"research\", \"knowledge\", \"experiment\"],\n",
    "    \"Children\": [\"children\", \"kids\", \"juvenile\", \"childhood\", \"youth\", \"picture book\"],\n",
    "    \"Poetry\": [\"poetry\", \"verse\", \"rhyme\", \"lyrical\", \"poem\", \"stanza\"],\n",
    "}\n",
    "\n",
    "# Define a UDF to assign genres based on triggers.\n",
    "# The UDF assign_genre iterates through the genre_triggers dictionary.\n",
    "# For each genre, it iterates through the list of triggers associated with that genre.\n",
    "# It checks if any of these triggers (converted to lowercase for case-insensitive matching) are found in the 'about' text of the author.\n",
    "# If a trigger is found in the 'about' text, the UDF returns the associated genre.\n",
    "def assign_genre(text):\n",
    "    for genre, triggers in genre_triggers.items():\n",
    "        for trigger in triggers:\n",
    "            if trigger in text.lower():\n",
    "                return genre\n",
    "    return None\n",
    "\n",
    "# Register the UDF with Spark\n",
    "assign_genre_udf = udf(assign_genre, StringType())\n",
    "\n",
    "# Create a new column 'genre' based on the 'about' field\n",
    "authors_df = authors_df.withColumn(\"genre\", when(col(\"about\").isNotNull(), assign_genre_udf(col(\"about\"))).otherwise(None))\n",
    "\n",
    "# Create a new column 'genre' based on the 'description' field\n",
    "books_df = books_df.withColumn(\"genre\", when(col(\"description\").isNotNull(), assign_genre_udf(col(\"description\"))).otherwise(None))\n",
    "\n",
    "# Select and show the resulting DataFrame with 'id', 'name', and 'genre' columns\n",
    "result_authors_df = authors_df.select(\"id\", \"name\", \"genre\")\n",
    "result_authors_df.show()\n",
    "\n",
    "# Select and show the resulting DataFrame with 'id', 'title', and 'genre' columns\n",
    "result_books_df = books_df.select(\"id\", \"title\", \"genre\")\n",
    "result_books_df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points 2.3: If you feel comfortable and did both tasks, analyze the matching of the two procedures. You grouped books and authors in genres. Do these two procedures show correspondence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, if we increase the number of triggers for genres, the accuracy of dedicating of a genre would be higher.\n",
    "- After reading two datasets using PySpart, we defined an Object with some key value pairs. the keys are the geners that we want to dedicate to each book or author's description or about part. the values are the list of words that we search for them in the description and the about text.\n",
    "- then, we defined a User-Defined Function (UDF) to assign genres to authors and books based on certain \"triggers\" found in their 'about' and 'description' text.\n",
    "- A UDF is a way to define a custom function that can be applied to the columns of a PySpark DataFrame.\n",
    "- Then, We add a new column to the authors and books datafram, and wrote down the genres.\n",
    "- The same approach works fine for both of the frameworks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework 2 - AWS Question (AWSQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags\n",
      "romance            6001\n",
      "fiction            5291\n",
      "young-adult        5016\n",
      "fantasy            3666\n",
      "science-fiction    2779\n",
      "Name: count, dtype: int64\n",
      "CPU times: user 30.7 s, sys: 59.7 s, total: 1min 30s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "list_json = pd.read_json('list.json', lines = True, chunksize=10000)\n",
    "list_df = pd.concat(list_json, ignore_index=False)\n",
    "\n",
    "# Explode the 'tags' column to create separate rows for each tag\n",
    "exploded_tags = list_df['tags'].explode()\n",
    "\n",
    "# Count the frequency of each tag\n",
    "tag_counts = exploded_tags.value_counts()\n",
    "\n",
    "# Get the top 5 most frequently used tags\n",
    "top_5_tags = tag_counts.head(5)\n",
    "\n",
    "# Print the result\n",
    "print(top_5_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The information about the config of the EC2 instance\n",
    "first I created a instance:\n",
    "in Name and tags filed, I wrote:\n",
    "linux-t3-large\n",
    "in ‘Application and OS Images (Amazon Machine Image)’ I select Ubuntu Server 22.04 LTS\n",
    "\n",
    "as instance type, I selected: \n",
    "t3.large\n",
    "\n",
    "as Key pair, I selected my existing key that I used to connect.\n",
    "\n",
    "In Network settings, I chose:\n",
    "Select existing security group\n",
    "\n",
    "as Storage (volumes), I select 30 Size (GiB)\n",
    "\n",
    "Then pressed ‘Launch Instance’. \n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "## The command used to connect to the EC2\n",
    "\n",
    "I used the following command to connect to my EC2 linux server (instance):\n",
    "\n",
    "$ ssh -i \"myKeyPair-Arman.pem\" ubuntu@ec2-54-204-98-221.compute-1.amazonaws.com\n",
    "\n",
    "First I uploaded the file in my S3 bucket.\n",
    "\n",
    "in EC2, I ran the following command to download from S3 bucket. \n",
    "$ wget https://arman-feili.s3.amazonaws.com/list.json\n",
    "\n",
    "In order to run Jupyter Notebook in AWS linux server, I ran the following command in linux terminal:\n",
    "$ jupyter notebook --no-browser\n",
    "\n",
    "I ran the below command in my local terminal:\n",
    "$ ssh -L 8088:localhost:8888 -i \"myKeyPair-Arman.pem\" ubuntu@ec2-54-204-98-221.compute-1.amazonaws.com\n",
    "\n",
    "in order to see the Jupyter notebook window in a Graphical user interface, I ran: \n",
    "http://localhost:8088\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "## The commands used to upload the files and run the script on the EC2 instance through your local system\n",
    "\n",
    "I created a S3 bucket as an extra storage for EC2 instance.\n",
    "\n",
    "I clicked on ‘S3’ service.\n",
    "then pressed ‘create bucket’ and defined a name which was unique.\n",
    "\n",
    "for the AWS Region, chose:\n",
    "US East (N. Virginia) us-east-1\n",
    "\n",
    "then went to permissions\n",
    "> Block public access (bucket settings)\n",
    "> Edit\n",
    "unchecked all the blocks, so your bucket would be public.\n",
    "\n",
    "Went to permissions > Object OwnershipInfo > Edit\n",
    "change to ‘ACLs enabled’\n",
    "\n",
    "I uploaded the list.json file into the S3\n",
    "I changed the permission for Read and Write for everyone, so I could easily execute the following command to download list.json file from S3 bucket\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "## A table containing the most popular tags and their number of usage\n",
    "\n",
    "tags\n",
    "romance:            6001,\n",
    "fiction:            5291,\n",
    "young-adult:        5016,\n",
    "fantasy:            3666,\n",
    "science-fiction:    2779\n",
    "\n",
    "### Tags:\n",
    "| romance | fiction | young-adult | fantasy | science-fiction\n",
    "| --- | --- | --- | --- | --- |\n",
    "| 6001 | 5291 | 5016 | 3666 | 2779 |\n",
    "\n",
    "-----------------------------------------------------------------------\n",
    "\n",
    "## A table containing the running time of the script on your local system and EC2 instance\n",
    "\n",
    "local machine:\n",
    "CPU times: user 30.7 s, sys: 59.7 s, total: 1min 30s\n",
    "Wall time: 2min 13s\n",
    "\n",
    "EC2 instance:\n",
    "It ran out of memory and the Kernel of Jupyter died.\n",
    "I ran $htop to see the server condition.\n",
    "both 2 cores of the CPU were using their upmost capacity and\n",
    "the 8 GIB of RAM was compeletly full.\n",
    "\n",
    "I tried to install different EC2 instances, including: linux-t3-xlarge, linux-t3-2xlarge, and linux-t3-3xlarge.\n",
    "Due to limitation, we only have access to such server with 8 GIB of Ram. However, this task requires a server with 16 GIB of RAM and at least 4 cores of CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./t3-large.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM Homework 2 - Algorithmic Question (AQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pops(left_shelf, right_shelf, book_id):\n",
    "    # Check if the book is on the left shelf.\n",
    "    if book_id in left_shelf:\n",
    "        # If the book is on the left shelf, find its index.\n",
    "        left_pops = left_shelf.index(book_id)  # O(n), where n is the number of books on the left shelf\n",
    "    else:\n",
    "        # If the book is not on the left shelf, set left_pops to the number of books on the left shelf.\n",
    "        left_pops = len(left_shelf)  # O(1)\n",
    "\n",
    "    # Check if the book is on the right shelf.\n",
    "    if book_id in right_shelf:\n",
    "        # If the book is on the right shelf, find its index.\n",
    "        right_pops = len(right_shelf) - right_shelf.index(book_id) - 1  # O(n), where n is the number of books on the right shelf\n",
    "    else:\n",
    "        # If the book is not on the right shelf, set right_pops to the number of books on the right shelf.\n",
    "        right_pops = len(right_shelf)  # O(1)\n",
    "\n",
    "    # Calculate the minimum pops between left_pops and right_pops.\n",
    "    minimum_pops = min(left_pops, right_pops)  # O(1)\n",
    "\n",
    "    # Return the minimum pops value.\n",
    "    return minimum_pops\n",
    "# Overall time complexity is O(n) in the worst case\n",
    "\n",
    "def print_instruction_result():\n",
    "    # Initialize the left and right shelves.\n",
    "    left_shelf = []  # O(1)\n",
    "    right_shelf = []  # O(1)\n",
    "    \n",
    "    # Read the number of instructions.\n",
    "    n = int(input(\"How many instructions do you want to apply? \"))  # O(1)\n",
    "\n",
    "    # Initialize a list to store the results of type 3 instructions.\n",
    "    results = []\n",
    "    \n",
    "    # Process each instruction.\n",
    "    for i in range(n):  # O(n)\n",
    "        # Read the instruction and split it into action and book_id.\n",
    "        instruction = input(f\"instruction # {i+1}: \").split()\n",
    "        action, book_id = instruction[0], int(instruction[1])\n",
    "\n",
    "        if action == 'L':\n",
    "            # If it's 'L', insert the book at the beginning of the left shelf.\n",
    "            left_shelf.insert(0, book_id)  # O(n) - Total number of instructions is 'n'.\n",
    "\n",
    "        elif action == 'R':\n",
    "            # If it's 'R', append the book to the right shelf.\n",
    "            right_shelf.append(book_id)  # O(1)\n",
    "\n",
    "        else:\n",
    "            # we calculate the minimum pops, which involves searching for the book's position in both the left and right shelf lists. \n",
    "            # The worst-case time complexity for searching in a list is O(n), and in the worst case, we perform two searches. \n",
    "            # Therefore, the time complexity for calculating minimum pops is O(n).\n",
    "            \n",
    "            # Type 3 instruction, calculate and append the result to the results list.\n",
    "            # Calculate the minimum pops by calling the 'calculate_pops' function.\n",
    "            result = calculate_pops(left_shelf, right_shelf, book_id)  # O(n) - Searching in lists with worst-case time complexity.\n",
    "\n",
    "            # Append the result to the results list.\n",
    "            results.append(result)\n",
    "    \n",
    "    # Print the results for type 3 instructions.\n",
    "    print(f\"The results of {n} instructions are: \")\n",
    "\n",
    "    # Iterate through the results list and print each result.\n",
    "    for result in results:  # O(n)\n",
    "        print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many instructions do you want to apply?  8\n",
      "instruction # 1:  L 75\n",
      "instruction # 2:  R 20\n",
      "instruction # 3:  R 30\n",
      "instruction # 4:  L 11\n",
      "instruction # 5:  ? 75\n",
      "instruction # 6:  L 12\n",
      "instruction # 7:  L 15\n",
      "instruction # 8:  ? 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of 8 instructions are: \n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print_instruction_result()\n",
    "# Input 1\n",
    "\n",
    "# 8\n",
    "# L 75\n",
    "# R 20\n",
    "# R 30\n",
    "# L 11\n",
    "# ? 75\n",
    "# L 12\n",
    "# L 15\n",
    "# ? 20\n",
    "\n",
    "# Output 1\n",
    "\n",
    "# 1\n",
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many instructions do you want to apply?  17\n",
      "instruction # 1:  R 1\n",
      "instruction # 2:  L 2\n",
      "instruction # 3:  L 3\n",
      "instruction # 4:  L 4\n",
      "instruction # 5:  ? 3\n",
      "instruction # 6:  R 5\n",
      "instruction # 7:  R 6\n",
      "instruction # 8:  L 7\n",
      "instruction # 9:  L 8\n",
      "instruction # 10:  ? 4\n",
      "instruction # 11:  L 9\n",
      "instruction # 12:  R 10\n",
      "instruction # 13:  R 11\n",
      "instruction # 14:  L 12\n",
      "instruction # 15:  L 13\n",
      "instruction # 16:  ? 11\n",
      "instruction # 17:  ? 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The results of 17 instructions are: \n",
      "1\n",
      "2\n",
      "0\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print_instruction_result()\n",
    "\n",
    "# Input 2\n",
    "\n",
    "# 17\n",
    "# R 1\n",
    "# L 2\n",
    "# L 3\n",
    "# L 4\n",
    "# ? 3\n",
    "# R 5\n",
    "# R 6\n",
    "# L 7\n",
    "# L 8\n",
    "# ? 4\n",
    "# L 9\n",
    "# R 10\n",
    "# R 11\n",
    "# L 12\n",
    "# L 13\n",
    "# ? 11\n",
    "# ? 3\n",
    "\n",
    "# Output 2:\n",
    "\n",
    "# 1\n",
    "# 2\n",
    "# 0\n",
    "# 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the time complexity (the Big O notation):\n",
    "\n",
    "The overall time complexity of the print_instruction_result function is dominated by the loop iterating 'n' times, and the time complexity is $O(n^2)$ in the worst case. This is due to the $O(n)$ time complexity for inserting into the left shelf for each 'L' instruction, and the $O(n)$ time complexity for calculating minimum pops in '3' instructions. \n",
    "\n",
    "Overall time complexity (the Big $O$ notation) is the maximum time complexity of the whole code. which is : $O(n^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is the proposed algorithm optimal?\n",
    "\n",
    "The algorithm proposed in the minimum_pops function is not the most optimal one.\n",
    "it performs a linear search in both left_shelf and right_shelf to find the position of the book with 'book_id'.\n",
    "A more efficient approach would be using a dictionary or a set, to keep track of the positions of books in each shelf.\n",
    "The following code is a revised minimum_pops method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimum_pops(left_shelf, right_shelf, book_id):\n",
    "    # Create dictionaries to store book positions in left_shelf and right_shelf.\n",
    "    left_positions = {book: index for index, book in enumerate(left_shelf)}\n",
    "    right_positions = {book: index for index, book in enumerate(right_shelf)}\n",
    "    \n",
    "    # Find the position of the book with book_id in left_shelf, default to len(left_shelf) if not found.\n",
    "    left_pops = left_positions.get(book_id, len(left_shelf))\n",
    "    \n",
    "    # Find the position of the book with book_id in right_shelf, default to len(right_shelf) if not found.\n",
    "    right_pops = right_positions.get(book_id, len(right_shelf))\n",
    "    \n",
    "    # Calculate the minimum pops between left_pops and right_pops.\n",
    "    minimum_pops = min(left_pops, right_pops)\n",
    "    \n",
    "    return minimum_pops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This revised code results in an overall time complexity of $O(1)$ for the minimum_pops function.\n",
    "\n",
    "So in this case, the overall time complexity (the Big $O$ notation) of the whole code would be : $O(n)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
